<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="白广通的博客"><title>机器学习基石第五讲：Training versus Testing 笔记 | Bruce's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习基石第五讲：Training versus Testing 笔记</h1><a id="logo" href="/.">Bruce's Blog</a><p class="description">Stay hungry, stay foolish.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-book"> 历史</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习基石第五讲：Training versus Testing 笔记</h1><div class="post-meta">Feb 18, 2017<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2017/02/18/mlfound-5/" href="/2017/02/18/mlfound-5/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#recap-and-preview"><span class="toc-number">1.</span> <span class="toc-text">Recap and Preview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#effective-number-of-lines"><span class="toc-number">2.</span> <span class="toc-text">Effective Number of Lines</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#effective-number-of-hypotheses"><span class="toc-number">3.</span> <span class="toc-text">Effective Number of Hypotheses</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#break-point"><span class="toc-number">4.</span> <span class="toc-text">Break Point</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#summary"><span class="toc-number">5.</span> <span class="toc-text">Summary</span></a></li></ol></div></div><div class="post-content"><p>我们今天的话题是，training和testing到底有什么不同。要解决的根本问题是：机器学习究竟为什么是可行的，当hypothesis set是无限大时，究竟发生了什么事情。</p>
<a id="more"></a>
<h2 id="recap-and-preview">Recap and Preview</h2>
<p>从上节课中，我们知道机器学习的流程已经是这样： <img src="http://oky5aqxds.bkt.clouddn.com/5-1.png" alt="图5-1"></p>
<p>其中<span class="math inline">\(P\)</span>表示，我们的训练数据和测试数据都是由<span class="math inline">\(P\)</span>控制产生的。</p>
<p>光在训练数据上表现好还不够，我们希望在没有看过的资料上表现好才是我们想要的。</p>
<p>现在我们来回顾前四节课的重点： <img src="http://oky5aqxds.bkt.clouddn.com/5-2.png" alt="图5-2"></p>
<p>于是，我们把learning拆成了两个重要的问题：</p>
<ul>
<li>我们能否保证<span class="math inline">\(E_{out}(g)\)</span>和<span class="math inline">\(E_{in}(g)\)</span>是足够接近的</li>
<li>我们能否使<span class="math inline">\(E_{in}(g)\)</span>足够小</li>
</ul>
<p>而且前面我们主要关注hypothesis set <span class="math inline">\(H\)</span>是有限大的情况，那么<span class="math inline">\(H\)</span>的大小<span class="math inline">\(M\)</span>在以上这两个问题中究竟扮演什么角色呢？</p>
<p>上面的两个问题在<span class="math inline">\(M\)</span>上的trade-off如下： <img src="http://oky5aqxds.bkt.clouddn.com/5-3.png" alt="图5-3"></p>
<p>我们能够看出，<span class="math inline">\(M\)</span>的选择对learning是非常重要的，太大或太小都不好。那么这样一来<span class="math inline">\(M = \infty\)</span>的情形不是注定效果很差吗？</p>
<p>我们这里要想办法解决无限大的<span class="math inline">\(M\)</span>发生什么事。我们这里的思路是，用一个有限的数量<span class="math inline">\(m_{H}\)</span>来代替<span class="math inline">\(M\)</span>，这里用<span class="math inline">\(m\)</span>表示比原来的<span class="math inline">\(M\)</span>小，而下标<span class="math inline">\(H\)</span>表示是跟<span class="math inline">\(H\)</span>的性质是有一定关系的： <img src="http://oky5aqxds.bkt.clouddn.com/5-4.png" alt="图5-4"></p>
<p>这里打一个问号，是因为我们还不知道可不可行。倘若我们能够找到这样一个<span class="math inline">\(m_{H}\)</span>的话，我们就能：</p>
<ul>
<li>证明当<span class="math inline">\(M\)</span>无限大时，learning是可行的</li>
<li>利用<span class="math inline">\(m_{H}\)</span>来帮助我们选择<span class="math inline">\(H\)</span>，就像<span class="math inline">\(M\)</span>一样</li>
</ul>
<p>这是一个偏理论性的问题，我们要花差不多3节课的课程来证明PLA是完全合理的。</p>
<p>这节课的测验题是： <img src="http://oky5aqxds.bkt.clouddn.com/5-5.png" alt="图5-5"></p>
<p>这里，我们可以推导出<span class="math inline">\(N\)</span>关于其他变量的公式： <span class="math display">\[ N = \frac{1}{2\epsilon^2} ln{\frac{2M}{\delta}}\]</span></p>
<h2 id="effective-number-of-lines">Effective Number of Lines</h2>
<p>现在我们从finite bin model出发，讨论一下，为什么<span class="math inline">\(M = \infty\)</span>时没办法做。 <img src="http://oky5aqxds.bkt.clouddn.com/5-6.png" alt="图5-6"></p>
<p>因此，如果每一个<span class="math inline">\(P[B_{m}]\)</span>都大于0的话，那么这无限个概率加起来，会远远超过1，那么这个bound也就不会有任何意义。</p>
<p>所以这个bound到底出了什么问题？我们用union bound的出发点是：对不同的hypothesis来讲，坏事不重叠（或者说不怎么重叠）。但是实际上，对两个相似的hypothesis，其<span class="math inline">\(BAD\,DATASET\)</span>其实是重叠的： <img src="http://oky5aqxds.bkt.clouddn.com/5-7.png" alt="图5-7"></p>
<p>图中，右边是对不同的hypothesis重叠的bad event的一种形象的表示。因此，在这种情况下，union-bound是对真实bound的一种<strong>过分估计（over-estimating）</strong>。</p>
<p>那为了对付这种重叠的情况，第一步就是把无限的hypothesis分成有限多类，每一类里是差不多的hypothesis。</p>
<p>我们先举个例子，我们假设平面上所有的线、所有的perceptron就是我们的hypothesis set。那么：</p>
<ul>
<li>有无限多条线（<span class="math inline">\(\infty\)</span>）</li>
<li>但是如果只有1个输入向量<span class="math inline">\(\mathbf{x}_1\)</span>，那么显然在我们的眼中只有两种线： <img src="http://oky5aqxds.bkt.clouddn.com/5-8.png" alt="图5-8"></li>
<li>那有2个点的时候呢？按照我们之前的推理方法，一共有4种： <img src="http://oky5aqxds.bkt.clouddn.com/5-9.png" alt="图5-9"></li>
<li>那么有3个点的时候呢？
<ul>
<li>如果是三角形的情形，也就是OX的每一种组合都线性可分的情况下，是有8种组合的。</li>
<li>如果是三点共线的情形，则只有6种线：（在共线或重叠的情况下，都会少于8种） <img src="http://oky5aqxds.bkt.clouddn.com/5-10.png" alt="图5-10"></li>
</ul></li>
<li>那么4个点的时候呢？
<ul>
<li>如果是四边形的情形，会有1种及其对称的做不到： <img src="http://oky5aqxds.bkt.clouddn.com/5-11.png" alt="图5-11"><br>
之所以乘2，是因为其他那7种是OX完全对称的，将直线两侧调换一下即可。</li>
</ul></li>
</ul>
<p>从前面的推导，我们知道，如果从有限的数据看出去，我们的线的种类是有限的。这个种类的个数，我们称之为<strong>“effective number of lines”</strong>：</p>
<div class="figure">
<img src="http://oky5aqxds.bkt.clouddn.com/5-12.png" alt="图5-12">
<p class="caption">图5-12</p>
</div>
<p>我们可以知道，这个effective number of lines一定是不会超过<span class="math inline">\(2^N\)</span>的，表示这个数字是有限的。如果我们可以用这个数字取代<span class="math inline">\(M\)</span>： <span class="math display">\[ P[|E_{in}(g) - E_{out}(g)| &gt; \epsilon] \le 2*effective(N)exp(-2\epsilon^2N)  \]</span><br>
那么当：</p>
<ul>
<li><span class="math inline">\(effective(N)\)</span>可以代替<span class="math inline">\(M\)</span></li>
<li>且<span class="math inline">\(effective(N) &lt;&lt; 2^N\)</span>时，</li>
</ul>
<p>当N增大到很大时，上式右面的bound就会趋向于0。这样即使是无限条线，我们也可以说我们学到了东西。这暂时只是我们的猜想，我们要花一些力气，才能证明这是对的。</p>
<h2 id="effective-number-of-hypotheses">Effective Number of Hypotheses</h2>
<p>接下来我们来看，如果我们用线的话，究竟有几种不一样的线。如果我们将来用高维度的hyperplane等的话，又应该怎么说明究竟有几种hypothesis这种事。</p>
<p>我们称： <span class="math display">\[ h(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N) = (h(\mathbf{x}_1), h(\mathbf{x}_2), \ldots, h(\mathbf{x}_N)) \in \{x, o\}^N\]</span> 为一个<strong>dichotomy</strong>（二分，意为把数据集分成两半），相当于只从<span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\)</span>的角度来看hypothesis的不同。</p>
<p>而<span class="math inline">\(H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)\)</span>则表示H能在<span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\)</span>上实现的所有dichotomy。下表是hypothesis和dichotomy的一个对比： <img src="http://oky5aqxds.bkt.clouddn.com/5-13.png" alt="图5-13"></p>
<p>于是，<span class="math inline">\(|H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)|\)</span>就是我们用来替代M的一个选项。</p>
<p>我们可以看到，我们的<span class="math inline">\(|H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)|\)</span>其实是依赖于我们的输入<span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\)</span>的。为了消除这个影响，我们选择dichotomy set最大的那一个就好了： <span class="math display">\[m_H(N) = \max_{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N \in X}{|H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)|}\]</span> 我们称这个函数为<strong>growth function（成长函数）</strong>，是跟我们的hypothesis set有关的。通过前面，我们知道，这个函数的输出一定是有限的，即<span class="math inline">\(\le 2^N\)</span>。</p>
<p>对perceptron的hypothesis set来说，写出来这个growth function会有点难，我们先来看一些简单的例子：</p>
<ul>
<li>Positive Ray (1-D perceptron的一半)：<span class="math inline">\(m_H(N) = N+1 &lt;&lt; 2^N\)</span> <img src="http://oky5aqxds.bkt.clouddn.com/5-14.png" alt="图5-14"></li>
<li>Positive Interval：<span class="math inline">\(m_H(N) = {N+1 \choose 2} + 1 = \frac{1}{2}N^2 + \frac{1}{2}N + 1 &lt;&lt; 2^N\)</span> <img src="http://oky5aqxds.bkt.clouddn.com/5-15.png" alt="图5-15"></li>
<li>Convex Set：<span class="math inline">\(m_H(N) = 2^N\)</span> <img src="http://oky5aqxds.bkt.clouddn.com/5-16.png" alt="图5-16"> 我们称我们的N个输入被H <strong>shattered（击倒）</strong>，当且仅当我们能找到N个点（可能是特别的N个点），其上的<span class="math inline">\(2^N\)</span>个dichotomy都可以被H做出来，那么此时<span class="math inline">\(m_H(N) = 2^N\)</span>。</li>
</ul>
<h2 id="break-point">Break Point</h2>
<p>我们现在已知的growth function有：</p>
<ul>
<li>Positive Ray: <span class="math inline">\(m_H(N) = N+1\)</span></li>
<li>Positive Interval: <span class="math inline">\(m_H(N) = {N+1 \choose 2} + 1\)</span></li>
<li>Convex Set: <span class="math inline">\(m_H(N) = 2^N\)</span></li>
<li>2D Perceptron: <span class="math inline">\(m_H(N) &lt; 2^N\)</span>在某些情况下</li>
</ul>
<p>如果我们现在用<span class="math inline">\(m_H(N)\)</span>取代<span class="math inline">\(M\)</span>的话，会怎样呢？ <span class="math display">\[ P[|E_{in}(g) - E_{out}(g)| &gt; \epsilon] \le 2*m_{H}(N)exp(-2\epsilon^2N)  \]</span> 我们可以得知，如果<span class="math inline">\(m_H(N)\)</span>是多项式的话，就是好的，因为其增长速度小于后面那一项；而如果<span class="math inline">\(m_H(N)\)</span>是指数式的话，就是不好的，因为和后面那一项的增长速度一个量级，我们不能确保N够大时，<span class="math inline">\(E_{in}\)</span>和<span class="math inline">\(E_{out}\)</span>很接近。那么2D Perceptron到底是好的，还是不好的呢？</p>
<p>我们再对growth function多做一个小小的定义。对一个hypothesis set H，我们定义k为它的<strong>break point</strong>，当且仅当k使得<span class="math inline">\(m_{H}(k)&lt;2^k\)</span>。可想而知，k+1, k+2, …也都是break point。我们只关注minimum break point。对2d perceptron， min break point就是4.</p>
<p>对于之前讨论过的一些growth function，它们对应的break point分别是： <img src="http://oky5aqxds.bkt.clouddn.com/5-17.png" alt="图5-17"></p>
<p>从上面图中，我们产生了一个猜想：</p>
<ul>
<li>当没有break point时，<span class="math inline">\(m_H(N) = 2^N\)</span></li>
<li><strong>当有break point时，<span class="math inline">\(m_H(N) = O(N^{k-1})\)</span></strong></li>
</ul>
<p>那我们的猜想究竟对不对呢？且看下次课程。</p>
<h2 id="summary">Summary</h2>
<div class="figure">
<img src="http://oky5aqxds.bkt.clouddn.com/5-18.png" alt="图5-18">
<p class="caption">图5-18</p>
</div>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://gtbai.github.io/2017/02/18/mlfound-5/" data-id="cj1c60311000its8fjfjv5isv" class="article-share-link">分享到</a><div class="tags"><a href="/tags/机器学习基石/">机器学习基石</a></div><div class="post-nav"><a href="/2017/02/19/mlfound-6/" class="pre">机器学习基石第六讲：Theory of Generalization 笔记</a><a href="/2017/02/14/mlfound-4/" class="next">机器学习基石第四讲：Learning is Impossible 笔记</a></div><div id="disqus_thread"><script>var disqus_shortname = 'gtbai';
var disqus_identifier = '2017/02/18/mlfound-5/';
var disqus_title = '机器学习基石第五讲：Training versus Testing 笔记';
var disqus_url = 'http://gtbai.github.io/2017/02/18/mlfound-5/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//gtbai.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/日常技巧/">日常技巧</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">8</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Sublime-Text/" style="font-size: 15px;">Sublime Text</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/04/10/Sublime-Text中的Project/">Sublime Text中的Project</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/22/mlfound-8/">机器学习基石第八讲：Noise and Error 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/20/mlfound-7/">机器学习基石第七讲：The VC Dimension 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/mlfound-6/">机器学习基石第六讲：Theory of Generalization 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/mlfound-5/">机器学习基石第五讲：Training versus Testing 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/14/mlfound-4/">机器学习基石第四讲：Learning is Impossible 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/11/mlfound-3/">机器学习基石第三讲：Types of Learning 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/09/mlfound-2/">机器学习基石第二讲：Learning to Answer Yes/No 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/06/mlfound-1/">机器学习基石第一讲：The Learning Problem 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/05/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//gtbai.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Bruce's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><br/><span id="busuanzi_container_site_pv"><i class="fa fa-mouse-pointer"></i><span> </span><span rel="nofollow" id="busuanzi_value_site_pv"></span></span><span> | </span><span id="busuanzi_container_site_uv"><i class="fa fa-user-circle-o"></i><span> </span><span rel="nofollow" id="busuanzi_value_site_uv"></span></span></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script></div></body></html>