<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[机器学习基石第五讲：Training versus Testing 笔记]]></title>
      <url>http://gtbai.github.io/2017/02/18/mlfound-5/</url>
      <content type="html"><![CDATA[<p>我们今天的话题是，training和testing到底有什么不同。要解决的根本问题是：机器学习究竟为什么是可行的，当hypothesis set是无限大时，究竟发生了什么事情。</p>
<a id="more"></a>
<h2 id="recap-and-preview">Recap and Preview</h2>
<p>从上节课中，我们知道机器学习的流程已经是这样： <img src="http://oky5aqxds.bkt.clouddn.com/5-1.png" alt="图5-1"></p>
<p>其中<span class="math inline">\(P\)</span>表示，我们的训练数据和测试数据都是由<span class="math inline">\(P\)</span>控制产生的。</p>
<p>光在训练数据上表现好还不够，我们希望在没有看过的资料上表现好才是我们想要的。</p>
<p>现在我们来回顾前四节课的重点： <img src="http://oky5aqxds.bkt.clouddn.com/5-2.png" alt="图5-2"></p>
<p>于是，我们把learning拆成了两个重要的问题：</p>
<ul>
<li>我们能否保证<span class="math inline">\(E_{out}(g)\)</span>和<span class="math inline">\(E_{in}(g)\)</span>是足够接近的</li>
<li>我们能否使<span class="math inline">\(E_{in}(g)\)</span>足够小</li>
</ul>
<p>而且前面我们主要关注hypothesis set <span class="math inline">\(H\)</span>是有限大的情况，那么<span class="math inline">\(H\)</span>的大小<span class="math inline">\(M\)</span>在以上这两个问题中究竟扮演什么角色呢？</p>
<p>上面的两个问题在<span class="math inline">\(M\)</span>上的trade-off如下： <img src="http://oky5aqxds.bkt.clouddn.com/5-3.png" alt="图5-3"></p>
<p>我们能够看出，<span class="math inline">\(M\)</span>的选择对learning是非常重要的，太大或太小都不好。那么这样一来<span class="math inline">\(M = \infty\)</span>的情形不是注定效果很差吗？</p>
<p>我们这里要想办法解决无限大的<span class="math inline">\(M\)</span>发生什么事。我们这里的思路是，用一个有限的数量<span class="math inline">\(m_{H}\)</span>来代替<span class="math inline">\(M\)</span>，这里用<span class="math inline">\(m\)</span>表示比原来的<span class="math inline">\(M\)</span>小，而下标<span class="math inline">\(H\)</span>表示是跟<span class="math inline">\(H\)</span>的性质是有一定关系的： <img src="http://oky5aqxds.bkt.clouddn.com/5-4.png" alt="图5-4"></p>
<p>这里打一个问号，是因为我们还不知道可不可行。倘若我们能够找到这样一个<span class="math inline">\(m_{H}\)</span>的话，我们就能：</p>
<ul>
<li>证明当<span class="math inline">\(M\)</span>无限大时，learning是可行的</li>
<li>利用<span class="math inline">\(m_{H}\)</span>来帮助我们选择<span class="math inline">\(H\)</span>，就像<span class="math inline">\(M\)</span>一样</li>
</ul>
<p>这是一个偏理论性的问题，我们要花差不多3节课的课程来证明PLA是完全合理的。</p>
<p>这节课的测验题是： <img src="http://oky5aqxds.bkt.clouddn.com/5-5.png" alt="图5-5"></p>
<p>这里，我们可以推导出<span class="math inline">\(N\)</span>关于其他变量的公式： <span class="math display">\[ N = \frac{1}{2\epsilon^2} ln{\frac{2M}{\delta}}\]</span></p>
<h2 id="effective-number-of-lines">Effective Number of Lines</h2>
<p>现在我们从finite bin model出发，讨论一下，为什么<span class="math inline">\(M = \infty\)</span>时没办法做。 <img src="http://oky5aqxds.bkt.clouddn.com/5-6.png" alt="图5-6"></p>
<p>因此，如果每一个<span class="math inline">\(P[B_{m}]\)</span>都大于0的话，那么这无限个概率加起来，会远远超过1，那么这个bound也就不会有任何意义。</p>
<p>所以这个bound到底出了什么问题？我们用union bound的出发点是：对不同的hypothesis来讲，坏事不重叠（或者说不怎么重叠）。但是实际上，对两个相似的hypothesis，其<span class="math inline">\(BAD\,DATASET\)</span>其实是重叠的： <img src="http://oky5aqxds.bkt.clouddn.com/5-7.png" alt="图5-7"></p>
<p>图中，右边是对不同的hypothesis重叠的bad event的一种形象的表示。因此，在这种情况下，union-bound是对真实bound的一种<strong>过分估计（over-estimating）</strong>。</p>
<p>那为了对付这种重叠的情况，第一步就是把无限的hypothesis分成有限多类，每一类里是差不多的hypothesis。</p>
<p>我们先举个例子，我们假设平面上所有的线、所有的perceptron就是我们的hypothesis set。那么：</p>
<ul>
<li>有无限多条线（<span class="math inline">\(\infty\)</span>）</li>
<li>但是如果只有1个输入向量<span class="math inline">\(\mathbf{x}_1\)</span>，那么显然在我们的眼中只有两种线： <img src="http://oky5aqxds.bkt.clouddn.com/5-8.png" alt="图5-8"></li>
<li>那有2个点的时候呢？按照我们之前的推理方法，一共有4种： <img src="http://oky5aqxds.bkt.clouddn.com/5-9.png" alt="图5-9"></li>
<li>那么有3个点的时候呢？
<ul>
<li>如果是三角形的情形，也就是OX的每一种组合都线性可分的情况下，是有8种组合的。</li>
<li>如果是三点共线的情形，则只有6种线：（在共线或重叠的情况下，都会少于8种） <img src="http://oky5aqxds.bkt.clouddn.com/5-10.png" alt="图5-10"></li>
</ul></li>
<li>那么4个点的时候呢？
<ul>
<li>如果是四边形的情形，会有1种及其对称的做不到： <img src="http://oky5aqxds.bkt.clouddn.com/5-11.png" alt="图5-11"><br>
之所以乘2，是因为其他那7种是OX完全对称的，将直线两侧调换一下即可。</li>
</ul></li>
</ul>
<p>从前面的推导，我们知道，如果从有限的数据看出去，我们的线的种类是有限的。这个种类的个数，我们称之为<strong>“effective number of lines”</strong>：</p>
<div class="figure">
<img src="http://oky5aqxds.bkt.clouddn.com/5-12.png" alt="图5-12">
<p class="caption">图5-12</p>
</div>
<p>我们可以知道，这个effective number of lines一定是不会超过<span class="math inline">\(2^N\)</span>的，表示这个数字是有限的。如果我们可以用这个数字取代<span class="math inline">\(M\)</span>： <span class="math display">\[ P[|E_{in}(g) - E_{out}(g)| &gt; \epsilon] \le 2*effective(N)exp(-2\epsilon^2N)  \]</span><br>
那么当：</p>
<ul>
<li><span class="math inline">\(effective(N)\)</span>可以代替<span class="math inline">\(M\)</span></li>
<li>且<span class="math inline">\(effective(N) &lt;&lt; 2^N\)</span>时，</li>
</ul>
<p>当N增大到很大时，上式右面的bound就会趋向于0。这样即使是无限条线，我们也可以说我们学到了东西。这暂时只是我们的猜想，我们要花一些力气，才能证明这是对的。</p>
<h2 id="effective-number-of-hypotheses">Effective Number of Hypotheses</h2>
<p>接下来我们来看，如果我们用线的话，究竟有几种不一样的线。如果我们将来用高维度的hyperplane等的话，又应该怎么说明究竟有几种hypothesis这种事。</p>
<p>我们称： <span class="math display">\[ h(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N) = (h(\mathbf{x}_1), h(\mathbf{x}_2), \ldots, h(\mathbf{x}_N)) \in \{x, o\}^N\]</span> 为一个<strong>dichotomy</strong>（二分，意为把数据集分成两半），相当于只从<span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\)</span>的角度来看hypothesis的不同。</p>
<p>而<span class="math inline">\(H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)\)</span>则表示H能在<span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\)</span>上实现的所有dichotomy。下表是hypothesis和dichotomy的一个对比： <img src="http://oky5aqxds.bkt.clouddn.com/5-13.png" alt="图5-13"></p>
<p>于是，<span class="math inline">\(|H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)|\)</span>就是我们用来替代M的一个选项。</p>
<p>我们可以看到，我们的<span class="math inline">\(|H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)|\)</span>其实是依赖于我们的输入<span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\)</span>的。为了消除这个影响，我们选择dichotomy set最大的那一个就好了： <span class="math display">\[m_H(N) = \max_{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N \in X}{|H(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)|}\]</span> 我们称这个函数为<strong>growth function（成长函数）</strong>，是跟我们的hypothesis set有关的。通过前面，我们知道，这个函数的输出一定是有限的，即<span class="math inline">\(\le 2^N\)</span>。</p>
<p>对perceptron的hypothesis set来说，写出来这个growth function会有点难，我们先来看一些简单的例子：</p>
<ul>
<li>Positive Ray (1-D perceptron的一半)：<span class="math inline">\(m_H(N) = N+1 &lt;&lt; 2^N\)</span> <img src="http://oky5aqxds.bkt.clouddn.com/5-14.png" alt="图5-14"></li>
<li>Positive Interval：<span class="math inline">\(m_H(N) = {N+1 \choose 2} + 1 = \frac{1}{2}N^2 + \frac{1}{2}N + 1 &lt;&lt; 2^N\)</span> <img src="http://oky5aqxds.bkt.clouddn.com/5-15.png" alt="图5-15"></li>
<li>Convex Set：<span class="math inline">\(m_H(N) = 2^N\)</span> <img src="http://oky5aqxds.bkt.clouddn.com/5-16.png" alt="图5-16"> 我们称我们的N个输入被H <strong>shattered（击倒）</strong>，当且仅当我们能找到N个点（可能是特别的N个点），其上的<span class="math inline">\(2^N\)</span>个dichotomy都可以被H做出来，那么此时<span class="math inline">\(m_H(N) = 2^N\)</span>。</li>
</ul>
<h2 id="break-point">Break Point</h2>
<p>我们现在已知的growth function有：</p>
<ul>
<li>Positive Ray: <span class="math inline">\(m_H(N) = N+1\)</span></li>
<li>Positive Interval: <span class="math inline">\(m_H(N) = {N+1 \choose 2} + 1\)</span></li>
<li>Convex Set: <span class="math inline">\(m_H(N) = 2^N\)</span></li>
<li>2D Perceptron: <span class="math inline">\(m_H(N) &lt; 2^N\)</span>在某些情况下</li>
</ul>
<p>如果我们现在用<span class="math inline">\(m_H(N)\)</span>取代<span class="math inline">\(M\)</span>的话，会怎样呢？ <span class="math display">\[ P[|E_{in}(g) - E_{out}(g)| &gt; \epsilon] \le 2*m_{H}(N)exp(-2\epsilon^2N)  \]</span> 我们可以得知，如果<span class="math inline">\(m_H(N)\)</span>是多项式的话，就是好的，因为其增长速度小于后面那一项；而如果<span class="math inline">\(m_H(N)\)</span>是指数式的话，就是不好的，因为和后面那一项的增长速度一个量级，我们不能确保N够大时，<span class="math inline">\(E_{in}\)</span>和<span class="math inline">\(E_{out}\)</span>很接近。那么2D Perceptron到底是好的，还是不好的呢？</p>
<p>我们再对growth function多做一个小小的定义。对一个hypothesis set H，我们定义k为它的<strong>break point</strong>，当且仅当k使得<span class="math inline">\(m_{H}(k)&lt;2^k\)</span>。可想而知，k+1, k+2, …也都是break point。我们只关注minimum break point。对2d perceptron， min break point就是4.</p>
<p>对于之前讨论过的一些growth function，它们对应的break point分别是： <img src="http://oky5aqxds.bkt.clouddn.com/5-17.png" alt="图5-17"></p>
<p>从上面图中，我们产生了一个猜想：</p>
<ul>
<li>当没有break point时，<span class="math inline">\(m_H(N) = 2^N\)</span></li>
<li><strong>当有break point时，<span class="math inline">\(m_H(N) = O(N^{k-1})\)</span></strong></li>
</ul>
<p>那我们的猜想究竟对不对呢？且看下次课程。</p>
<h2 id="summary">Summary</h2>
<div class="figure">
<img src="http://oky5aqxds.bkt.clouddn.com/5-18.png" alt="图5-18">
<p class="caption">图5-18</p>
</div>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[机器学习基石第四讲：Learning is Impossible 笔记]]></title>
      <url>http://gtbai.github.io/2017/02/14/mlfound-4/</url>
      <content type="html"><![CDATA[<p>今天要探讨的问题是Feasibility of Learning，即Learning是不是可行的。我们上次埋了一个梗，说Learning搞不好是做不到的。我们先讨论这个问题，再研究即使Learning是做不到的，我们能否加上一些假设或者使用一些方法，使Learning是做得到的。</p>
<a id="more"></a>
<h2 id="learning-is-impossible">Learning is Impossible?</h2>
<p>老师首先展示了一个有趣的Human Learning的小例子： <img src="http://oky5aqxds.bkt.clouddn.com/4-1.png" alt="图4-1"></p>
<p>这里老师是想通过例子告诉大家，如果机器学习问题没有限制的话，那么无论你学到什么东西，你的“敌人”永远可以说你是没有学到东西。这样看起来，学习是一件不可能的任务。</p>
<p>这里还有一个简单的二元分类的问题： <img src="http://oky5aqxds.bkt.clouddn.com/4-2.png" alt="图4-2"></p>
<p>那就算我们找到的<span class="math inline">\(g\)</span>在<span class="math inline">\(D\)</span>中提供的5组3-bit<span class="math inline">\((x, y)\)</span>上和<span class="math inline">\(f\)</span>上完全相同，在<span class="math inline">\(D\)</span>以外，我们却仍相当于没有学到任何东西： <img src="http://oky5aqxds.bkt.clouddn.com/4-3.png" alt="图4-3"></p>
<p>也就是说，如果任何“未知”的<span class="math inline">\(f\)</span>都有可能发生的话，从已知资料<span class="math inline">\(D\)</span>中进行学习将注定失败。</p>
<h2 id="probability-to-the-rescue">Probability to the rescue</h2>
<p>刚才我们说到，在非常严格的环境下，learning是做不到的。但是我们可以想一些办法，用一些工具，去对未知的东西做一些推论。</p>
<p>想象我们有一个大大的罐子，其中有很多橘色的弹珠和绿色的弹珠。如果我们想要知道罐子中橘色弹珠占的比例。如果我们没有办法一颗一颗拿出来数，我们可以怎么做呢？或者说，我们怎么来<strong>估计</strong>一下橘色弹珠的比例呢？</p>
<p>当然有办法，我们可以使用抽样的方法： <img src="http://oky5aqxds.bkt.clouddn.com/4-4.png" alt="图4-4"></p>
<p>那in-sample的比例<span class="math inline">\(\nu\)</span>会不会告诉我们一些关于out-sample的比例<span class="math inline">\(\mu\)</span>的信息呢？</p>
<ul>
<li>不会
<ul>
<li>因为即使罐子里大部分是橘色弹珠，你也有可能拿到一把绿色弹珠。（虽然几率很小，但有可能发生）</li>
</ul></li>
<li>会
<ul>
<li>因为有很大可能in-sample <span class="math inline">\(\nu\)</span> 会和未知的<span class="math inline">\(\mu\)</span>相近。</li>
</ul></li>
</ul>
<p>那么在数学上，我们怎么正式地表达这件事呢？</p>
<p>在一个很大的样本里，大致上来说<span class="math inline">\(\nu\)</span>和<span class="math inline">\(\mu\)</span>很接近： <span class="math display">\[ P[|\nu - \mu| &gt; \epsilon] \le 2exp(-2\epsilon^2N)\]</span></p>
<p>这被称为<strong>Hoeffding不等式</strong>，用在弹珠比例估计，丢硬币的正反面出现概率估计，利用民意调查估计民意等。通俗点说，<span class="math inline">\(\nu = \mu\)</span>”大概、差不多“是对的（Probably Approximately Correct, PAC）。”大概“表示概率大，”差不多“表示值相差不大。</p>
<p>Hoeffding不等式：</p>
<ul>
<li>对所有<span class="math inline">\(N\)</span>和<span class="math inline">\(\epsilon\)</span>成立</li>
<li>不需要知道<span class="math inline">\(\mu\)</span>，也没有任何关于<span class="math inline">\(\mu\)</span>的假设</li>
<li><span class="math inline">\(N\)</span>越大，<span class="math inline">\(\epsilon\)</span>越大，<span class="math inline">\(\nu \approx \mu\)</span>的几率越大</li>
</ul>
<p>也就是说，当<span class="math inline">\(N\)</span>够大的话，我们是可以从已知的<span class="math inline">\(\nu\)</span>推论未知的<span class="math inline">\(\mu\)</span>的。</p>
<h2 id="connection-to-learning">Connection to Learning</h2>
<p>那我们前面说的这些，和learning有什么关系呢？下面是bin model和machine learning中元素的一一对应： <img src="http://oky5aqxds.bkt.clouddn.com/4-5.png" alt="图4-5"></p>
<p>这样，我们就得到了这么一个推论：当<span class="math inline">\(N\)</span>够大而且<span class="math inline">\(\mathbf{x}_n\)</span>是从<span class="math inline">\(X\)</span>中i.i.d.地抽出时，我们可以大概地通过已知<span class="math inline">\(D\)</span>上<span class="math inline">\([h(\mathbf{x}_n) \ne f(\mathbf{x}_n)]\)</span>的比例推断出未知的整个<span class="math inline">\(X\)</span>上<span class="math inline">\([h(\mathbf{x}) \ne f(\mathbf{x})]\)</span>的概率。</p>
<p>这幅图展示了在机器学习的流程中一些附加的要素： <img src="http://oky5aqxds.bkt.clouddn.com/4-6.png" alt="图4-6"></p>
<p>其中的概率分布<span class="math inline">\(P\)</span>决定了我们的数据集<span class="math inline">\(D\)</span>中的<span class="math inline">\(\mathbf{x}_n\)</span>是如何从<span class="math inline">\(X\)</span>中产生的，就像决定了我们在从罐子里抓弹珠时，抓的是哪一把弹珠。我们并不需要知道它。</p>
<p>根据上图，对任意一个固定的<span class="math inline">\(h\)</span>，我们可以大概地推测：</p>
<ul>
<li>未知的 <span class="math inline">\(E_{out}(h) = \varepsilon_{\mathbf{x} \sim P} [h(\mathbf{x}) \ne f(\mathbf{x})]\)</span></li>
<li>通过已知的 <span class="math inline">\(E_{in}(h) = \frac{1}{N} \sum_{n=1}^{N} \|h(\mathbf{x}_n) \ne y_n \|\)</span></li>
</ul>
<p>其中out是指out-sample，是指在未知的数据点；in是指in-sample，是指在已知的数据集中。</p>
<p>把Hoeffding定理代过来，我们有：对任意固定的<span class="math inline">\(h\)</span>，在“大量”数据中，我们有：in-sample的误差<span class="math inline">\(E_{in}(h)\)</span>大概和out-sample的误差<span class="math inline">\(E_{out}(h)\)</span>接近（误差在<span class="math inline">\(\epsilon\)</span>以内）： <span class="math display">\[ P[|E_{in}(h) - E_{out}(h)| &gt; \epsilon] \le 2exp(-2\epsilon^2N)\]</span></p>
<p>和“罐子”比喻相同，这里也有：</p>
<ul>
<li>对所有<span class="math inline">\(N\)</span>和<span class="math inline">\(\epsilon\)</span>成立</li>
<li>不需要知道<span class="math inline">\(E_{out}(h)\)</span>，也没有任何关于<span class="math inline">\(E_{out}(h)\)</span>的假设
<ul>
<li>不需要知道<span class="math inline">\(f\)</span>和<span class="math inline">\(P\)</span></li>
</ul></li>
<li>“$ E_{in}(h) = E_{out}(h) $”是“大概、差不多”正确（PAC）</li>
</ul>
<p>也就是说，如果”$ E_{in}(h) E_{out}(h) <span class="math inline">\(“而且”\)</span>E_{in}(h)<span class="math inline">\(比较小“，那么可以推出\)</span>E_{out}(h)也比较小<span class="math inline">\(。也就是在同一个\)</span>P<span class="math inline">\(产生的数据点上，\)</span>h f$。</p>
<p>那么对于任意固定的<span class="math inline">\(h\)</span>，当数据够多的时候： <span class="math display">\[ E_{in}(h) \approx E_{out}(h) \]</span> 我们可以说我们学到了好的结果吗？</p>
<ul>
<li>一个回答是可以
<ul>
<li>当<span class="math inline">\(E_{in}(h)\)</span>对一个固定的<span class="math inline">\(h\)</span>很小的时候，而且算法<span class="math inline">\(A\)</span>恰好选择了<span class="math inline">\(h\)</span>作为输出<span class="math inline">\(g\)</span>，那么”$ g = f$“ PAC。</li>
</ul></li>
<li>另一个回答是不可以
<ul>
<li>因为我们的定理只是针对某一个固定的<span class="math inline">\(h\)</span>有bound，对不同的<span class="math inline">\(h\)</span>之间并没有约束。所以这就造成了对不同的数据集，算法<span class="math inline">\(A\)</span>总是会选择<span class="math inline">\(h\)</span>作为<span class="math inline">\(g\)</span>，然而：
<ul>
<li><span class="math inline">\(E_{in}(h)\)</span>几乎总是不小的</li>
<li>恰恰使得”$ g f$“ PAC。</li>
</ul></li>
</ul></li>
</ul>
<p>因此，真正的学习是：<span class="math inline">\(A\)</span>应该能够自己从<span class="math inline">\(H\)</span>中做选择，而不是被强迫去选哪一个<span class="math inline">\(h\)</span>。</p>
<p>因此，我们现在做到的不能说是learning，只能说是verification： <img src="http://oky5aqxds.bkt.clouddn.com/4-7.png" alt="图4-7"></p>
<p>也就是说，我们现在只能够做到验证某个<span class="math inline">\(h\)</span>的好坏，但还不能做到从<span class="math inline">\(H\)</span>中选出最好的那个<span class="math inline">\(h\)</span>出来。</p>
<h2 id="connection-to-real-learning">Connection to Real Learning</h2>
<p>前面我们说到一个<span class="math inline">\(h\)</span>时我们可以做verification，那多个hypothesis时我们怎么办呢？</p>
<p>如果每一个hypothesis我们让它对应一个罐子，假设其中一个罐子中抽出来的弹珠全是绿色的，也就是说其中一个hypothesis在我们的<span class="math inline">\(D\)</span>上全对，那么我们要不要选它呢？</p>
<p>假设在台大ML课上，150个同学每个同学都丢5次硬币，其中一位同学的硬币<span class="math inline">\(g\)</span>出现了连续5次正面。那么这就说明这位同学的<span class="math inline">\(g\)</span>真的比其他同学的好吗？</p>
<p>从概率论的角度解释，显然不是：至少有一个硬币出现连续5次正面的概率为<span class="math inline">\(1 - (\frac{31}{32})^{150} &gt; 0.99\)</span>。</p>
<p>这件事告诉我们：在我们<em>做选择</em>时，”bad sample“的影响可能会恶化。所谓bad sample，就是Hoeffding定理中的例外情况，即<span class="math inline">\(E_{in}\)</span>和<span class="math inline">\(E_{out}\)</span>差的比较多的情况。比如在丢一个硬币时，你以为它比别的硬币好的概率是<span class="math inline">\(\frac{1}{32}\)</span>，但是有150颗硬币时，你找到一颗你认为比别的更好的硬币的概率却大于0.99。</p>
<p>从不好的硬币，我们来看不好的数据：</p>
<ul>
<li>不好的硬币：<span class="math inline">\(E_{out} = \frac{1}{2}, E_{in} = 0\)</span></li>
<li>不好的数据：<span class="math inline">\(E_{out}\)</span>和<span class="math inline">\(E_{in}\)</span>差的很远：
<ul>
<li>例如<span class="math inline">\(E_{out}\)</span>很大，而<span class="math inline">\(E_{in}\)</span>却很小（在很多例子中成立）， 例如： <img src="http://oky5aqxds.bkt.clouddn.com/4-8.png" alt="图4-8"> 其中的<span class="math inline">\(D_{n}\)</span>表示第<span class="math inline">\(n\)</span>把抽出来的数据。</li>
</ul></li>
</ul>
<p>那如果有很多的<span class="math inline">\(h\)</span>，所谓的不好的数据是怎么样呢？其实就是，它阻碍了算法<span class="math inline">\(A\)</span>“自由”地做选择。而好的数据，则是可以让算法<span class="math inline">\(A\)</span>自由地做选择，选什么都是对的。通俗一点讲，不好的数据是有可能让<span class="math inline">\(A\)</span>踩到雷。而”有雷“，就是指至少存在一个<span class="math inline">\(h\)</span>使得<span class="math inline">\(E_{out}(h)\)</span>和<span class="math inline">\(E_{in}(h)\)</span>差的很远： <img src="http://oky5aqxds.bkt.clouddn.com/4-9.png" alt="图4-9"></p>
<p>因此，我们想在有<span class="math inline">\(M\)</span>个hypothesis的情况下，为<span class="math inline">\(P_{D}[BAD \, D]\)</span>找出一个bound。</p>
<p><span class="math display">\[
\begin{align}
    &amp; P_{D}[BAD\,D] \\
 = &amp; P_{D}[BAD \, D \, for \, h_1 \, or \, BAD \, D \, for \, h_2 \, or \, \cdots \, or \, BAD \, D \, for \, h_M] \\  
 \le &amp; P_{D}[BAD \, D \, for \, h_1] + P_{D}[BAD \, D \, for \, h_2] + \cdots + P_{D}[BAD \, D \, for \, h_M] (union \, bound) \\
 \le &amp; 2exp(-2\epsilon^2N) + 2exp(-2\epsilon^2N) + \cdots + 2exp(-2\epsilon^2N) \\
 = &amp; 2Mexp(-2\epsilon^2N)   
\end{align}
\]</span></p>
<p>如同bin model的Hoeffding定理一样，这里finite-bin版本的Hoeffding定理有：</p>
<ul>
<li>对所有的<span class="math inline">\(M, N, \epsilon\)</span>成立</li>
<li>不依赖于任何一个<span class="math inline">\(E_{out}(h_m)\)</span>，不需要知道<span class="math inline">\(E_{out}(h_m)\)</span>，因此也就不需要知道<span class="math inline">\(f\)</span>和<span class="math inline">\(P\)</span></li>
<li>可以说<span class="math inline">\(E_{in}(g) = E_{out}(g)\)</span> PAC，对任何一个算法<span class="math inline">\(A\)</span>都成立</li>
</ul>
<p>那么看起来，最合理的演算法就是选一个<span class="math inline">\(E_{in}\)</span>最小的<span class="math inline">\(g\)</span>。</p>
<p>那么当<span class="math inline">\(|H| = M\)</span>且有限时，<span class="math inline">\(N\)</span>够大时，对<span class="math inline">\(A\)</span>挑选的任意<span class="math inline">\(g\)</span>，有<span class="math inline">\(E_{in}(g) \approx E_{out}(g)\)</span>。那么如果<span class="math inline">\(A\)</span>找到了一个<span class="math inline">\(E_{in}(g) \approx 0\)</span>， PAC可以保证<span class="math inline">\(E_{out} \approx 0\)</span>。那么更新之后的learning flow如下： <img src="http://oky5aqxds.bkt.clouddn.com/4-10.png" alt="图4-10"></p>
<p>但是很多hypothesis set都是无限大的，例如perceptron中的无限条线。因此，现在问题还是没有完全解决。但是这里的有限的hypothesis set给了我们一个一个很好的出发点，让我们知道了learning可以做到一些事情。而无限的情况有一些困难，可能还要花费两到三节课的时间来解决这个问题。</p>
<p>本节的测验题非常有趣，值得一看： <img src="http://oky5aqxds.bkt.clouddn.com/4-11.png" alt="图4-11"></p>
<h2 id="summary">Summary</h2>
<div class="figure">
<img src="http://oky5aqxds.bkt.clouddn.com/4-12.png" alt="图4-12">
<p class="caption">图4-12</p>
</div>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[机器学习基石第三讲：Types of Learning 笔记]]></title>
      <url>http://gtbai.github.io/2017/02/11/mlfound-3/</url>
      <content type="html"><![CDATA[<p>今天我们从上次的Yes/No题出发，探讨一下机器学习中还有哪些不同的问题，它们与我们的Yes/No题又有哪些不一样，以及不一样在什么地方。</p>
<a id="more"></a>
<h2 id="learning-with-different-output">Learning with Different Output</h2>
<p>回顾一下上次的Yes/No问题： <img src="http://oky5aqxds.bkt.clouddn.com/3-1.png" alt="图3-1"></p>
<p>Yes/No题的应用非常的广泛：</p>
<ul>
<li>信用卡批准/不批准</li>
<li>邮件是垃圾邮件/不是垃圾邮件</li>
<li>病人生病了/没有生病</li>
<li>广告会盈利/不会盈利</li>
<li>学生对某道习题的回答正确/不正确（KDD Cup 2010）</li>
</ul>
<p>多元分类问题（以硬币分类为例子）： <img src="http://oky5aqxds.bkt.clouddn.com/3-2.png" alt="图3-2"></p>
<p>其他的多元分类问题：</p>
<ul>
<li>手写数字识别 <span class="math inline">\(\Rightarrow\)</span> 0, 1, 2, …, 9</li>
<li>图片识别 <span class="math inline">\(\Rightarrow\)</span> 水果，橘子，草莓</li>
<li>邮件分类 <span class="math inline">\(\Rightarrow\)</span> 垃圾邮件，重要邮件，社交邮件，促销邮件，更新邮件等</li>
</ul>
<p>其中一个应用非常多的领域就是recognition，即视觉/听觉的辨识。</p>
<p>在医院的情境下，二元分类可以告诉我们病人有没有生病，而多元分类可以告诉我们病人生的是哪一种病。而如果我们想要知道：病人多久后会康复。也就是说问题的输出是整个实数域，或者一个有界的区间，即： <span class="math display">\[ Y = R \quad or \quad Y = [lower, upper] \subset R \]</span></p>
<p>这样的问题就是<strong>回归</strong>问题。</p>
<p>其他的一些回归问题包括：</p>
<ul>
<li>根据公司数据预测股票价格</li>
<li>根据气候数据预测温度</li>
</ul>
<p>回归问题同分类问题一样，是机器学习中很重要、很基础的问题，可以用来做为其他问题的基础。</p>
<p>接下来有一个复杂的问题：自然语言中的词性标注问题。 <img src="http://oky5aqxds.bkt.clouddn.com/3-3.png" alt="图3-3"></p>
<p>这种问题被称为<strong>结构化学习问题</strong>，类似的问题还有：</p>
<ul>
<li>输入蛋白质的数据 <span class="math inline">\(\Rightarrow\)</span> 输出蛋白质的3D模型</li>
<li>输入一段话 <span class="math inline">\(\Rightarrow\)</span> 输出每一个字在这段话的关系是什么（Speech Parse Tree）</li>
</ul>
<p>简单总结：</p>
<ul>
<li><strong>二元分类</strong>：<span class="math inline">\(Y = {-1, +1}\)</span></li>
<li>多元分类：<span class="math inline">\(Y = {1,2, \cdots, K}\)</span></li>
<li><strong>回归</strong>：<span class="math inline">\(Y = R\)</span></li>
<li>结构化学习：<span class="math inline">\(Y = \{structures\}\)</span></li>
<li>其他</li>
</ul>
<h2 id="learning-with-different-data-label">Learning with Different Data Label</h2>
<p>我们现在从硬币分类问题出发：</p>
<p>如果我们输入每一个硬币的大小和重量，以及每一个硬币的价值，让机器训练出一个分类器，那么这就是一个<strong>监督式学习(Supervised Learning)</strong>问题： <img src="http://oky5aqxds.bkt.clouddn.com/3-5.png" alt="图3-5"></p>
<p>那如果我们不告诉机器每一个硬币的价值，让机器自己去分类，那么这是将是一个<strong>非监督式学习(Unsupervised Learning)</strong>，称为<strong>聚类问题（Clustering Problem）</strong>。这将是一个比分类更困难的问题，比如如何衡量聚类效果的好坏就很难。</p>
<p>典型的聚类问题有：</p>
<ul>
<li>把文章自动归到不同的分类</li>
<li>根据顾客的档案把他们分到不同的群</li>
</ul>
<p>非监督式学习的典型问题有：</p>
<ul>
<li><strong>聚类</strong>：<span class="math inline">\(\mathbf{x}_n \Rightarrow cluster(\mathbf{x})\)</span> (约等于Unsupervised Multiclass Classification）
<ul>
<li>把文章自动归到不同的分类</li>
</ul></li>
<li><strong>密度估计</strong>：<span class="math inline">\(\mathbf{x}_n \Rightarrow density(\mathbf{x})\)</span> (约等于Unsupervised Bounded Regression)
<ul>
<li>根据交通事故报告推测有经常发生事故的区域</li>
</ul></li>
<li><strong>异常检测</strong>：<span class="math inline">\(\mathbf{x}_n \Rightarrow unusual(\mathbf{x})\)</span> （约等于Unsupervised Binary Claasfication）
<ul>
<li>根据互联网的日志信息给出被入侵或者不正常运作警报</li>
<li>因为异常情况很少，取得labeled data也很难</li>
</ul></li>
<li>其他</li>
</ul>
<p><strong>半监督式学习</strong>（Semi-supervised）指在我们的输入数据集里，少部分数据有标签，大部分数据无标签。典型问题有：</p>
<ul>
<li>输入少量被标记的人脸图片 <span class="math inline">\(\Rightarrow\)</span> 人脸识别器（Facebook）</li>
<li>少量药效被测试过的药数据 <span class="math inline">\(\Rightarrow\)</span> 药效果预测器</li>
</ul>
<p>半监督式学习问题的一个共同特点是<strong>取得标签都很贵</strong>，于是想到用大量未标记数据来避免昂贵的标记工作。</p>
<p><strong>增强学习</strong>是一种“完全不同”但是自然的学习方式。</p>
<p>想一想你怎么训练你的宠物？</p>
<p>例如你叫你的狗坐下，狗却在地上尿尿：</p>
<ul>
<li>你不能直接告诉你的狗当<span class="math inline">\(\mathbf{x}_n = &#39;sit down&#39;\)</span>时，<span class="math inline">\(y_n = sit\)</span></li>
<li>但是你可以在它做错时惩罚它；</li>
<li>或者在它做对时奖励它，比如给它饼干吃。</li>
<li>当然也不一定是准确做到坐下时，才奖励它。比如它做了一个相似的动作像握手，你也还是会给它饼干吃，只不过可能不会给它吃它最爱吃的那种。</li>
</ul>
<p>这种通过在输出上施加奖惩的策略来进行学习的方式，叫做增强学习。其他的一些例子有：</p>
<ul>
<li>输入（顾客信息，系统的广告选择，广告点击带来的盈利）<span class="math inline">\(\Rightarrow\)</span> 通过学习进步了的广告投放系统</li>
<li>输入（手上的牌，系统的出牌策略，系统赢得的钱）<span class="math inline">\(\Rightarrow\)</span> 通过学习进步了的出牌系统</li>
</ul>
<p>增强学习中，我们并不会告诉机器对于某个输入准确的输出是什么，只会告诉它某个输出好还是不好，由机器按照这些信息进行学习。这些信息通常是序列化到达，而不是一起到达的。</p>
<p>简单总结：</p>
<ul>
<li><strong>监督式学习</strong>：所有<span class="math inline">\(x_n\)</span>都有<span class="math inline">\(y_n\)</span></li>
<li>非监督式学习：所有<span class="math inline">\(x_n\)</span>都没有<span class="math inline">\(y_n\)</span></li>
<li>半监督式学习：少部分<span class="math inline">\(x_n\)</span>有<span class="math inline">\(y_n\)</span></li>
<li>增强式学习：<span class="math inline">\(y_n\)</span>好还是不好（<span class="math inline">\(\tilde{y_n}\)</span>）</li>
</ul>
<h2 id="learning-with-different-protocol">Learning with Different Protocol</h2>
<p><strong>Batch Learning</strong>: 一次从所有已知数据中学习，是一种很常见的学习方式：</p>
<ul>
<li>输入一批（邮件，是否垃圾）的数据 <span class="math inline">\(\Rightarrow\)</span> 垃圾邮件过滤器</li>
<li>输入一批（病人，癌症种类）的数据 <span class="math inline">\(\Rightarrow\)</span> 癌症分类器</li>
<li>输入一批病人数据 <span class="math inline">\(\Rightarrow\)</span> 病人分组（聚类问题）</li>
</ul>
<p>这都是Batch Learning的学习方式。</p>
<p>与之相对的是<strong>线上学习（Online Learning）</strong>的方式：</p>
<ul>
<li>收到一封邮件</li>
<li>用已有的<span class="math inline">\(g_t\)</span>对它是否是垃圾邮件进行预测</li>
<li>用户告诉它正确的Label <span class="math inline">\(y_t\)</span>，系统根据<span class="math inline">\((\mathbf{x}_t, y_t)\)</span>更新<span class="math inline">\(g_t\)</span></li>
</ul>
<p>线上学习与我们之前学过问题有很多联系，例如：</p>
<ul>
<li>PLA问题可以很方便地采用线上学习方式</li>
<li>通常增强学习也是以线上学习的方式完成的</li>
</ul>
<p>通过线上学习，我们希望每一轮计算出的<span class="math inline">\(g\)</span>越变越好。</p>
<p>Batch Learning比较像填鸭式教育，线上学习比较像老师在教书。在这两个方式中，机器都是被动的。在最近的机器学习研究中，有一个热门的研究方向就是：我们能不能让机器自己问问题。比如在手写数字识别中，机器可以自己生成一个数字，或者拿一个它无法判断的数字，来问人这究竟是几。</p>
<p>这样的学习方式叫做<strong>主动学习(Active Learning)</strong>，希望通过让机器有策略地选择一些问题来问，从而用很少的标记来改善学习效果。通常也是用在数据很多或标记很贵的情境中。</p>
<p>简单总结：</p>
<ul>
<li><strong>batch learning</strong>: 一次性喂给机器所有已知数据</li>
<li>online learning: 序列性喂给机器数据</li>
<li>active learning: 向专家有策略地提问获得观察到的数据</li>
</ul>
<h2 id="learning-with-different-input-space">Learning with Different Input Space</h2>
<p>之前我们讲的都跟输出有关：输出的空间不同如何分类，取得标记的方法不同如何分类等等。现在我们来谈一谈输入。</p>
<p>还是之前的信用卡批准问题。我们其实假设我们的输入（用户资料）非常具体（concret），而且跟我们想要做的事有一定关系。更具体地说，它们可能都代表一些蛮复杂、经过处理的特征，这些特征和我们想要的输出有一些关系。</p>
<p>Concert代表，我们可能可以进行简单的运算就能得到一个能直接决定结果的变量（个人理解）。例如：</p>
<ul>
<li>硬币分类问题中，硬币的大小和重量</li>
<li>信用卡批准问题中，用户的具体信息</li>
<li>癌症诊断问题中，病人的具体信息</li>
</ul>
<p>在这些特征里面，带有人类的智慧对问题的描述（Domain Knowledge），人类的智慧已经用在像预处理的工作中，这类问题在机器学习中是比较简单的。</p>
<p>如果我们要做手写数字的识别，我们：</p>
<ul>
<li>可以输入数字是否对称，和数字的密度。（concret特征）： <img src="http://oky5aqxds.bkt.clouddn.com/3-8.png" alt="图3-8"></li>
<li>同时也可以直接输入16 x 16灰阶图像（raw特征）：每个点只有很简单的物理意义，更<strong>抽象</strong>一些： <img src="http://oky5aqxds.bkt.clouddn.com/3-9.png" alt="图3-9"></li>
</ul>
<p>很多情况下，需要人们把raw features转化成concrete features，这门学问叫做feature engineering。而深度学习(deep learning)，概略的来说，就是利用大量的数据，甚至使用非监督式的学习，来抽取具体的特征。</p>
<p>这些还不是最困难的，最困难的是非常非常抽象： 比如在KDD Cup 2011上的预测用户给歌曲打分问题：</p>
<ul>
<li>输入（userid, itemid, rating），预测某userid给某itemid的rating</li>
<li>典型的回归问题：<span class="math inline">\(Y \subseteq R, X \subseteq N x N\)</span></li>
<li>这个二维向量是没有实际意义的，因此对机器学习而言是一个很难的问题。</li>
<li>因此我们要为用户和歌曲抽取特征，这些特征可能一部分是人想的，另一部分是机器自己学到的。</li>
</ul>
<p>其他在KDD Cup里类似的问题：</p>
<ul>
<li>在线辅导系统里学生的编号(KDD Cup 2010)</li>
<li>在线广告投放系统中广告的编号</li>
</ul>
<p>这里的abstract feature比raw feature更抽象一层，难度更高一些，需要更多的抽取动作。</p>
<p>简单总结：</p>
<ul>
<li><strong>concret features</strong>: 有复杂的现实意义</li>
<li>raw features: 有简单的现实意义</li>
<li>abstract features: 没有（或很少）现实意义</li>
<li>其他</li>
</ul>
<h2 id="summary">Summary</h2>
<div class="figure">
<img src="http://oky5aqxds.bkt.clouddn.com/3-11.png" alt="图3-11">
<p class="caption">图3-11</p>
</div>
<p>下节课，我们将会讨论一个问题：<strong>机器学习可能是做不到的</strong>。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[机器学习基石第二讲：Learning to Answer Yes/No 笔记]]></title>
      <url>http://gtbai.github.io/2017/02/09/mlfound-2/</url>
      <content type="html"><![CDATA[<h2 id="perceptron-hypothesis-set">Perceptron Hypothesis Set</h2>
<p>信用卡批准问题回顾： <img src="http://oky5aqxds.bkt.clouddn.com/2-1.png" alt="图2-1"></p>
<p>那么对于这个问题，<span class="math inline">\(H\)</span>究竟长什么样呢？</p>
<p>这里，我们可以把每个用户表示成一个向量： <span class="math display">\[\mathbf{x} = (x_1, x_2, \cdots, x_d)\]</span></p>
<p>并计算一个加权的“分数”：</p>
<ul>
<li>如果<span class="math inline">\(\sum_{i=1}^dw_ix_i &gt; threshold\)</span>，那么批准信用卡的申请</li>
<li>如果<span class="math inline">\(\sum_i=1^dw_ix_i &lt; threshold\)</span>，那么拒绝信用卡的申请</li>
</ul>
<p>输出空间为<span class="math inline">\(Y: \{+1(good), -1(bad)\}\)</span>。边界情况0经常被我们忽略， 因为它很少发生，也不太重要。那么我们的假设就是一个线性函数<span class="math inline">\(h \epsilon H\)</span>： <span class="math display">\[h(x) = sign((\sum_{i=1}^dw_ix_i)-threshold)\]</span> 这个机器学习假设函数被叫做感知机（Perceptron），来源于早期类神经网络的研究者，因为很像人大脑中的一个神经元而得名。</p>
<p>这个<span class="math inline">\(h\)</span>形式略麻烦，这里从符号上进行简化： <img src="http://oky5aqxds.bkt.clouddn.com/2-2.png" alt="图2-2"></p>
<p>使得每一个向量<span class="math inline">\(\mathbf w\)</span>都表示一个假设函数<span class="math inline">\(h\)</span>，而每一个向量<span class="math inline">\(\mathbf x\)</span>都表示一个用户。</p>
<p>这么说起来还是很抽象，那么每一个<span class="math inline">\(h\)</span>到底长什么样呢？这里有一个二维的例子： <img src="http://oky5aqxds.bkt.clouddn.com/2-3.png" alt="图2-3"></p>
<h2 id="perceptron-learning-algorithm-pla">Perceptron Learning Algorithm (PLA)</h2>
<p>现在我们的任务就是从所有的感知机（线、平面等）中选出我们想要的<span class="math inline">\(g\)</span>：</p>
<ul>
<li>我们想要<span class="math inline">\(g \approx f\)</span>。但是很难，因为<span class="math inline">\(f\)</span>我们不知道</li>
<li>我们只知道我们的资料是从<span class="math inline">\(f\)</span>产生的，那我们可以要求在我们所拥有的数据<span class="math inline">\(D\)</span>上，有： <span class="math display">\[g(x_n) = f(x_n) = y_n\]</span></li>
<li>但是即使到这一步了，也不是太容易。因为<span class="math inline">\(H\)</span>无限大</li>
<li>那我们的idea是：从一个不太好的<span class="math inline">\(g_0\)</span>出发，然后不断地参考它在<span class="math inline">\(D\)</span>上的错误 来修正它。 接下来，我们会用<span class="math inline">\(\mathbf w_0\)</span>来代表<span class="math inline">\(g_0\)</span>。</li>
</ul>
<p>那么完整的PLA算法将是这样的： <img src="http://oky5aqxds.bkt.clouddn.com/2-4.png" alt="图2-4"> 其中更新的思路大概是这样：</p>
<ul>
<li>如果在某<span class="math inline">\(\mathbf x\)</span>处，其本身的label是<span class="math inline">\(y=1\)</span>，结果<span class="math inline">\(h\)</span>却输出<span class="math inline">\(y=-1\)</span>， 那么说明<span class="math inline">\(\mathbf w\)</span>和<span class="math inline">\(\mathbf x\)</span>的夹角太<strong>大</strong>，可以在<span class="math inline">\(\mathbf w\)</span>上加上 <span class="math inline">\(y\mathbf x\)</span>让它向<span class="math inline">\(\mathbf x\)</span><strong>靠近</strong>一些。</li>
<li>如果在某<span class="math inline">\(\mathbf x\)</span>处，其本身的label是<span class="math inline">\(y=-1\)</span>，结果<span class="math inline">\(h\)</span>却输出<span class="math inline">\(y=1\)</span>， 那么说明<span class="math inline">\(\mathbf w\)</span>和<span class="math inline">\(\mathbf x\)</span>的夹角太<strong>小</strong>，可以在<span class="math inline">\(\mathbf w\)</span>上加上 <span class="math inline">\(y\mathbf x\)</span>让它向<span class="math inline">\(\mathbf x\)</span><strong>远离</strong>一些。</li>
</ul>
<p>其中的哲学可以说是：</p>
<blockquote>
<p>知错能改，善莫大焉。</p>
</blockquote>
<p>常见的PLA实现方式有：</p>
<h3 id="循环pla">循环PLA</h3>
<ol style="list-style-type: decimal">
<li>对<span class="math inline">\(t=0,1,\ldots\)</span>
<ol style="list-style-type: decimal">
<li>找到<span class="math inline">\(\mathbf{w}_t\)</span>犯的<strong>下一个</strong>错误<span class="math inline">\((x_{n(t)}, y_{n(t)})\)</span> s.t. <span class="math display">\[sign(\mathbf w_t^T \mathbf x_{n(t)}) \neq y_{n(t)}\]</span></li>
<li>按照以下方式纠正这个错误： <span class="math display">\[\mathbf w_{t+1} \leftarrow \mathbf w_{t}+y_{n(t)}\mathbf x_{n(t)}\]</span></li>
</ol></li>
<li>直到在一轮完整的循环中找不到错误</li>
</ol>
<p>其中<strong>下一个</strong>既可以按照自然循环顺序<span class="math inline">\((1,2,\cdots,N)\)</span>来遍历，也可以通过提前计算的随机循环顺序来遍历。</p>
<p>既然演算法停的时候就会找出一个好的Perceptron，那么：</p>
<ul>
<li>算法一定会停吗？
<ul>
<li>按照自然循环会停吗？</li>
<li>按照随机循环会停吗？</li>
<li>其他的变种循环方式会停吗？</li>
</ul></li>
<li>以及，<span class="math inline">\(g\)</span>和<span class="math inline">\(f\)</span>像吗？
<ul>
<li>就算在训练集<span class="math inline">\(D\)</span>上想像，在<span class="math inline">\(D\)</span>以外像吗？</li>
<li>如果在<span class="math inline">\(D\)</span>上不停的话，<span class="math inline">\(g\)</span>和<span class="math inline">\(f\)</span>的关系又会怎么样呢？</li>
</ul></li>
</ul>
<p>我们接下来试图证明，在一些情况下，循环过足够多次数后，<strong>PLA总会停下来</strong>。</p>
<h2 id="guarantee-of-pla">Guarantee of PLA</h2>
<p><strong>线性可分</strong></p>
<blockquote>
<p>如果PLA能停下来，那么一个必要条件是数据集<span class="math inline">\(D\)</span>允许某个<span class="math inline">\(\mathbf{w}\)</span>不犯错误，此时我们称<span class="math inline">\(D\)</span>为<strong>线性可分</strong>。</p>
</blockquote>
<p>那么假设数据集<span class="math inline">\(D\)</span>是线性可分的，PLA一定会停吗？</p>
<p>线性可分 <span class="math inline">\(\Leftrightarrow\)</span> 存在一个完美的<span class="math inline">\(\mathbf w_f\)</span>使得<span class="math inline">\(y_n = sign(\mathbf w_f^T \mathbf x_n)\)</span>. <span class="math inline">\(\mathbf w_f\)</span>是完美的，因此每一个<span class="math inline">\(\mathbf x_n\)</span>都完美地远离分界线： <span class="math display">\[y_{n(t)}\mathbf w_f^T \mathbf x_{n(t)} &gt; \min_n y_n\mathbf w_f^T \mathbf x_n &gt; 0 \]</span> 通过对于任意的错误<span class="math inline">\((x_{n(t)}, y_{n(t)})\)</span>更新，我们有： <span class="math display">\[
    \begin{align}
        \mathbf{w}_f^T\cdot{\mathbf w_{t+1}} &amp; = \mathbf w_f^T(\mathbf w_t + y_{n(t)}\mathbf x_{n(t)}) \\
                    &amp; \ge \mathbf w_f^T\mathbf w_t + \min_{n}y_n\mathbf w_f^T\mathbf x_n \\
                    &amp; &gt; \mathbf{w}_f^T\mathbf{w}_t + 0
    \end{align}
    \]</span> 上面的结果实际上表明，每次纠正错误后，<span class="math inline">\(\mathbf{w}_f\)</span>和<span class="math inline">\(\mathbf{w}_t\)</span>的内积变大了。可能有两个原因：</p>
<ul>
<li><span class="math inline">\(\mathbf{w}_f\)</span>和<span class="math inline">\(\mathbf{w}_t\)</span>的夹角变小了。</li>
<li><span class="math inline">\(\mathbf{w}_t\)</span>的长度变大了。</li>
</ul>
<p>我们当然希望是第一种情况发生了，这样我们就能说明<span class="math inline">\(\mathbf{w}_t\)</span>在不断向<span class="math inline">\(\mathbf{w}_f\)</span>靠近。那么我们如何说明呢？</p>
<p>我们还没有利用一个性质，那就是$_t $ 在 <span class="math inline">\((\mathbf{x}_{n(t)}, y_{n(t)})\)</span>处犯了错误： <span class="math display">\[ sign(\mathbf{w}_t^T\mathbf{x}_{n(t)}) \neq y_{n(t)} \Leftrightarrow
       y_{n(t)}\mathbf{w}_t^T\mathbf{x}_{n(t)} \le 0 \]</span></p>
<p>我们可以利用这个性质证明：<span class="math inline">\(\mathbf{w}_t\)</span>其实变长的不太快，即使是关于<strong>最长的</strong><span class="math inline">\(\mathbf{x}_n\)</span>进行更新时： <span class="math display">\[
    \begin{align}
    {\|\mathbf{w}_{t+1}\|}^2 &amp; = {\|\mathbf{w}_{t} +y_{n(t)} \mathbf{x}_{n(t)}\|}^2 \\
                             &amp; = {\|\mathbf{w}_{t}\|}^2 + 2y_{n(t)}\mathbf{w}_{t}^T\mathbf{x}_{n(t)} + {\| y_{n(t)}\mathbf{x}_{n(t)} \|}^2 \\
                             &amp; \le {\|\mathbf{w}_{t}\|}^2 + 0 + {\| y_{n(t)}\mathbf{x}_{n(t)} \|}^2 \\
                             &amp; \le {\|\mathbf{w}_{t}\|}^2 + \max_n{\| y_n\mathbf{x}_n \|}^2 \\
                             &amp; \le {\|\mathbf{w}_{t}\|}^2 + \max_n{\|\mathbf{x}_n \|}^2
    \end{align}
    \]</span> 如此一来，我们便排除了第二种情况，说明了随着循环次数的增长，<span class="math inline">\(\mathbf{w}_f\)</span>和<span class="math inline">\(\mathbf{w}_t\)</span>的夹角不断变小。</p>
<p>这里老师还留了一个有趣的小问题。从<span class="math inline">\(\mathbf{w}_{0}=\mathbf{0}\)</span>开始，经过<span class="math inline">\(T\)</span>次错误纠正，我们有： <span class="math display">\[\frac{\mathbf{w}_f^T}{\|\mathbf{w}_f\|} \frac{\mathbf{w}_T}{\|\mathbf{w}_T\|} \ge \sqrt{T}\cdot{constant}\]</span> 问这个<span class="math inline">\(constant\)</span>应该是多少。 这个问题不算太难，通过上面的两个不等式，经过推导，我们可以得到： 
    $$constant = \frac {\min_n y_{n}\mathbf{w}_f^T\mathbf{x}_n} {{\|\mathbf{w}_{f}\|}^2 \max_n {\|\mathbf{x}_n \|}^2}$$
    </p>
<h2 id="non-seperable-data">Non-Seperable Data</h2>
<p>上面我们证明了，只要我们的数据是<strong>线性可分</strong>，且每次都<strong>修正一个错误</strong>：</p>
<ul>
<li><span class="math inline">\(\mathbf{w}_f\)</span>和<span class="math inline">\(\mathbf{w}_t\)</span><strong>的内积快速变大</strong>，而且<strong><span class="math inline">\(\mathbf{w}_t\)</span></strong>的长度增长的不快</li>
<li>也就是说：PLA这条线越来越接近<span class="math inline">\(\mathbf{w}_f\)</span>直到停止</li>
</ul>
<p>这样的好处是：<strong>实现很简单，对任何纬度<span class="math inline">\(d\)</span>都work。</strong></p>
<p>坏处是：</p>
<ul>
<li><strong>不知道会不会停下来</strong>（因为不知道<span class="math inline">\(\mathbf{w}_f\)</span>）。</li>
<li><strong>就算知道会停下来，也不知道多久会停下来</strong>（因为计算<span class="math inline">\(\rho\)</span>需要用到<span class="math inline">\(\mathbf{w}_f\)</span>）。</li>
</ul>
<p>那如果我们的数据集不是线性可分的话，我们怎么办呢？也就是用<strong>噪声数据</strong>进行学习： <img src="http://oky5aqxds.bkt.clouddn.com/2-5.png" alt="图2-5"></p>
<p>假设噪声是<strong>“小”</strong>的，即大部分情况下<span class="math inline">\(y_n = f(\mathbf{x}_n)\)</span>。 如果这样的话，那么我们也希望我们求得的<span class="math inline">\(g\)</span>在大部分情况下有<span class="math inline">\(y_n = g(\mathbf{x}_n)\)</span>。 于是，我们可以这样来求<span class="math inline">\(\mathbf{w}_g\)</span>： <span class="math display">\[\mathbf{w}_g \leftarrow \mathop{\arg\min}_{\mathbf{w}} \sum_{n=1}^{N}(y_n \neq sign(\mathbf{w}^T\mathbf{x}_n))\]</span> 但是很不幸，这个问题被证明是NP-hard的。</p>
<p>那么既然不好找到精确解，我们能不能通过修改PLA来近似地找到一个还能接受的g呢？</p>
<h3 id="pocket-algorithm">Pocket Algorithm</h3>
<blockquote>
<p>把最好的那条线抓在手上。</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>初始化Pocket权重为<span class="math inline">\(\hat{\mathbf{w}}\)</span></li>
<li>对<span class="math inline">\(t = 0, 1, \cdots\)</span>
<ol style="list-style-type: decimal">
<li>（随机）找到<span class="math inline">\(\mathbf{w}_t\)</span>所犯的一个错误<span class="math inline">\((\mathbf(x)_{n(t)}, y_{n(t)})\)</span></li>
<li>（尝试）通过以下方法修复错误： <span class="math display">\[ \mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t} + y_{n(t)}\mathbf{x}_{n(t)} \]</span></li>
<li>如果<span class="math inline">\(\mathbf{w}_{t+1}\)</span>比<span class="math inline">\(\hat{\mathbf{w}}\)</span>犯的错误还要少，那么我们用前者来替换后者。</li>
</ol></li>
</ol>
<p>直到经过了<strong>足够多次循环</strong>。 返回<span class="math inline">\(\hat{\mathbf{w}}\)</span>（我们称为<span class="math inline">\(\mathbf{w}_{POCKET}\)</span>）为我们所求的<span class="math inline">\(g\)</span>。</p>
<p>已有理论能够证明，如果数据集线性可分，那么PLA可以做得很好；如果不线性可分，那么Pocket Algorithm将会做得还不错。</p>
<h2 id="summary">Summary</h2>
<div class="figure">
<img src="http://oky5aqxds.bkt.clouddn.com/2-6.png" alt="图2-6">
<p class="caption">图2-6</p>
</div>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[机器学习基石第一讲：The Learning Problem 笔记]]></title>
      <url>http://gtbai.github.io/2017/02/06/mlfound-1/</url>
      <content type="html"><![CDATA[<p>此系列博客将记录我在学习<a href="http://www.csie.ntu.edu.tw/~htlin/mooc/" target="_blank" rel="external">《机器学习基石》</a>课程时的学习笔记。<a href="http://www.csie.ntu.edu.tw/~htlin/mooc/" target="_blank" rel="external">《机器学习基石》</a>课程由国立台湾大学林轩田老师在Cousera网站上开设，试图从概率论的角度介绍机器学习。我曾在16年夏季学期在Cousera网站上跟过此课程。当时因为懒惰，没有记录笔记。如今半年过去，课程内容已被我忘记大半。如今从头开始，认真整理笔记。目的是帮助我自己更好地理解和记忆课程知识，同时也希望能给学习本课程的同学提供一些帮助。</p>
<h1 id="第一讲the-learning-problem">第一讲：The Learning Problem</h1>
<h2 id="course-introduction">Course Introduction</h2>
<p>课程将会像讲故事一样，阐述四个问题：</p>
<ul>
<li><strong>When</strong> Can Machines Learn? (illustrative + technique)</li>
<li><strong>Why</strong> Can Machines Learn? (theoretical + illustrative)</li>
<li><strong>How</strong> Can Machines Learn? (technical + practical)</li>
<li>How Can Machines Learn <strong>Better</strong>? (practical + theoretical)</li>
</ul>
<h2 id="what-is-machine-learning">What is Machine Learning</h2>
<p>什么是学习： <img src="http://oky5aqxds.bkt.clouddn.com/1-1.png" alt="图1-1"> 什么是机器学习： <img src="http://oky5aqxds.bkt.clouddn.com/1-2.png" alt="图1-2"> 上面两个定义中都提到了<strong>技巧（Skill）</strong>这个概念，那么技巧是什么意思呢？</p>
<blockquote>
<p>技巧就是一种表现的增进（比如预测精度的增进）。</p>
</blockquote>
<p>这里提到了一个机器学习的应用场景：通过输入历史股票价格数据到机器，机器能够通过机器学习算法预测未来的股票走势，向我们建议好的投资方案。当我们按照这个方案去投资，就能获得更多的投资回报。</p>
<p>那么我们为什么要用机器学习，而不用其他理工科的工具呢？这里又有另一个机器学习的例子：辨识树。让你写一个程序，来辨识一个图里面有没有一棵树。那么如果你要用100条规则定义一棵树，这个任务将很难完成。而想一下我们小时候是怎么学习辨认树的：绝不是父母一下告诉我们100规则，然后我们通过这100条规则来辨认树。而是我们通过每天的观察，逐渐总结出来一些经验学习到的。这个例子也提醒我们：在完成有些任务（比如辨识树），特别是比较复杂的任务时，可能构造一个基于机器学习的系统比构造一个基于手写规则的系统更简单。</p>
<p>前面的例子启发我们，机器学习可能会应用在下列场景中：</p>
<ul>
<li>When human cannot program the system manually. 比如为一个探索火星的机器人编程。人们从来没有到过火星，不可能写程序来应对火星上所有的状况。</li>
<li>When human cannot ‘define the solution’ easily. 比如语音／视觉识别。人们可能无法准确定义出某一段声音信号是在由哪个字发出的。</li>
<li>When needing rapid decisions that humans cannot do. 比如股市超高频交易。人类不可能在10几秒中的时间内准确决定是要买还是要卖。</li>
<li>When needing to be user-oriented in a massive scale. 比如针对用户的市场服务。人们没办法通过写程序照顾到每一个不同顾客的个性化需求。</li>
</ul>
<blockquote>
<p>机器学习就像授机器以渔，而不是授机器以鱼。</p>
</blockquote>
<p>问题适合使用机器学习的三个关键特征：</p>
<ul>
<li>存在潜藏的模式可以学习 - 表现才可以被增进</li>
<li>但是又不知道怎么把它们写下来 - 才需要机器学习</li>
<li>有关于模式的资料 - 机器学习算法才有了输入</li>
</ul>
<h2 id="applications-of-machine-learning">Applications of Machine Learning</h2>
<p>孙中山先生说过：</p>
<blockquote>
<p>民生的需要，從前經濟學家都說是衣、食、住三種。照我的研究，應該有四種：於衣、食、住之外，還有一種就是行。…行就是走路。我們要解決民生問題，不但是要把這四種需要弄到很便宜，並且要全國人民都能夠享受。</p>
</blockquote>
<p>后来蒋介石先生又补充道：</p>
<blockquote>
<p>總理說過：『民生主義要做到少年的人有教育，壯年的人有職業，老年的人有養活，全國男女，無論老小，都可以享安樂』。所以，對於『育幼、養老、濟災、醫病與夫種種公共之需』，乃至『聾啞殘廢院，以濟大造之窮，公共花園，以供暇時之戲』，都要籌畫辦理，『把中國變成一個安樂的國家』，才是民生主義的完成。所以，我們如不把育、樂這兩問題，和食、衣、住、行這四個問題，一併提出研究，就不能概括　總理的民生主義的全部精神與目的之所在。</p>
</blockquote>
<p>因此，<strong>衣食住行育乐</strong>可谓是民生六大需求。而机器学习在这六方面都有所应用：</p>
<ul>
<li>食（Sadilek et al, 2013）
<ul>
<li>数据：推特数据（单词+位置）</li>
<li>技巧：在某个餐厅吃饭会不会食物中毒</li>
</ul></li>
<li>衣（Abu-Mostafa, 2012）
<ul>
<li>数据：销售数据+用户调查</li>
<li>技巧：向用户推荐时尚衣物搭配</li>
</ul></li>
<li>住（Tsanas and Xifara, 2012）
<ul>
<li>数据：房屋的特征和它们的能源消耗</li>
<li>技巧：预测房屋耗能</li>
</ul></li>
<li>行（Stallkamp et al, 2012）
<ul>
<li>数据：一些交通标志和它们的意义</li>
<li>技巧：准确识别交通标志</li>
</ul></li>
<li>育
<ul>
<li>数据：学生在一个数学辅导系统中的答题数据</li>
<li>技巧：预测学生是否会答对某道测验题</li>
<li>一个可能的机器学习算法：回答准确约等于【学生最近的水平 &gt; 题目的难度】
<ul>
<li>喂给数据来自3000名学生的900万条答题记录</li>
<li>机器自动决定学生水平和题目难度<br>
</li>
</ul></li>
</ul></li>
<li>乐 （电影推荐系统）
<ul>
<li>数据：用户给电影的评分</li>
<li>技巧：预测一个用户会怎样给一个系统打分
<ul>
<li>Netflix在20016年举办的比赛</li>
<li>Yahoo在2011年KDD Cup也办了一个类似的歌曲推荐比赛</li>
</ul></li>
</ul></li>
</ul>
<h2 id="components-of-machine-learning">Components of Machine Learning</h2>
<p>应用信息： <img src="http://oky5aqxds.bkt.clouddn.com/1-3.png" alt="图1-2"></p>
<p>学习目标：批准这个用户的信用卡申请对银行是否有益。</p>
<ul>
<li>输入: <span class="math inline">\(x \epsilon X\)</span>，用户申请</li>
<li>输出: <span class="math inline">\(y \epsilon Y\)</span>，批准申请是好／是坏</li>
<li>需要学习的潜在模式： <span class="math display">\[f: X \rightarrow Y \]</span> 理想的信用卡批准定理</li>
<li>数据&lt;-&gt;训练例子： <span class="math display">\[D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}\]</span> 学习到的信用卡批准定理</li>
</ul>
<p>问题符号化： <img src="http://oky5aqxds.bkt.clouddn.com/1-4.png" alt="图1-4"></p>
<p>可能的一些<span class="math inline">\(g \epsilon H = \{h_k\}\)</span>：</p>
<ul>
<li><span class="math inline">\(h_1\)</span>：年薪高于80万台币</li>
<li><span class="math inline">\(h_2\)</span>：负债大于10万台币</li>
<li><span class="math inline">\(h_3\)</span>: 参加工作不超过两年</li>
</ul>
<p>假设集合（hyposithesis set）<span class="math inline">\(H\)</span>:</p>
<ul>
<li>包含好的和坏的假设</li>
<li>由机器学习算法<span class="math inline">\(H\)</span>选择最好的<span class="math inline">\(g\)</span></li>
</ul>
<p>机器学习模型的定义：</p>
<blockquote>
<p>learning model = <span class="math inline">\(A\)</span> + <span class="math inline">\(H\)</span></p>
</blockquote>
<p>机器学习的实用定义： <img src="http://oky5aqxds.bkt.clouddn.com/1-5.png" alt="图1-5"></p>
<h2 id="machine-learning-and-other-fields">Machine Learning and Other Fields</h2>
<p>机器学习和数据挖掘的关系： <img src="http://oky5aqxds.bkt.clouddn.com/1-6.png" alt="图1-6"></p>
<p>机器学习和人工智能的关系： <img src="http://oky5aqxds.bkt.clouddn.com/1-7.png" alt="图1-7"></p>
<p>机器学习和统计学的关系： <img src="http://oky5aqxds.bkt.clouddn.com/1-8.png" alt="图1-8"></p>
<h2 id="summary">Summary</h2>
<p>什么时候机器可以学习？ <img src="http://oky5aqxds.bkt.clouddn.com/1-9.png" alt="图1-9"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>http://gtbai.github.io/2017/02/05/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    </entry>
    
  
  
</search>
